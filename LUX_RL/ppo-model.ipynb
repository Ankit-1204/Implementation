{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10349808,"sourceType":"datasetVersion","datasetId":6409013}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-21T18:46:57.585242Z","iopub.execute_input":"2025-01-21T18:46:57.585528Z","iopub.status.idle":"2025-01-21T18:47:01.686809Z","shell.execute_reply.started":"2025-01-21T18:46:57.585501Z","shell.execute_reply":"2025-01-21T18:47:01.685787Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lux-s3/lux-ai-season-3/agent.py\n/kaggle/input/lux-s3/lux-ai-season-3/main.py\n/kaggle/input/lux-s3/lux-ai-season-3/README.md\n/kaggle/input/lux-s3/lux-ai-season-3/lux/kit.py\n/kaggle/input/lux-s3/lux-ai-season-3/lux/utils.py\n/kaggle/input/lux-s3/lux-ai-season-3/lux/__init__.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade luxai_s3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T18:47:01.687849Z","iopub.execute_input":"2025-01-21T18:47:01.688267Z","iopub.status.idle":"2025-01-21T18:47:16.236854Z","shell.execute_reply.started":"2025-01-21T18:47:01.688234Z","shell.execute_reply":"2025-01-21T18:47:16.235690Z"}},"outputs":[{"name":"stdout","text":"Collecting luxai_s3\n  Downloading luxai_s3-0.2.0-py3-none-any.whl.metadata (253 bytes)\nRequirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from luxai_s3) (0.4.26)\nCollecting gymnax==0.0.8 (from luxai_s3)\n  Downloading gymnax-0.0.8-py3-none-any.whl.metadata (19 kB)\nCollecting tyro (from luxai_s3)\n  Downloading tyro-0.9.11-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (0.4.26+cuda12.cudnn89)\nRequirement already satisfied: chex in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (0.1.86)\nRequirement already satisfied: flax in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (0.8.4)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (6.0.2)\nCollecting gym>=0.26 (from gymnax==0.0.8->luxai_s3)\n  Downloading gym-0.26.2.tar.gz (721 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (0.29.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (3.7.1)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from gymnax==0.0.8->luxai_s3) (0.12.2)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->luxai_s3) (0.4.1)\nRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax->luxai_s3) (1.26.4)\nRequirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->luxai_s3) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->luxai_s3) (1.13.1)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from tyro->luxai_s3) (0.16)\nRequirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro->luxai_s3) (13.8.1)\nCollecting shtab>=1.5.6 (from tyro->luxai_s3)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from tyro->luxai_s3) (4.3.0)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from tyro->luxai_s3) (4.12.2)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.26->gymnax==0.0.8->luxai_s3) (3.1.0)\nRequirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.26->gymnax==0.0.8->luxai_s3) (0.0.8)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->luxai_s3) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->luxai_s3) (2.18.0)\nRequirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->gymnax==0.0.8->luxai_s3) (1.4.0)\nRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex->gymnax==0.0.8->luxai_s3) (0.12.1)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax->gymnax==0.0.8->luxai_s3) (1.0.8)\nRequirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax->gymnax==0.0.8->luxai_s3) (0.2.2)\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax->gymnax==0.0.8->luxai_s3) (0.6.4)\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax->gymnax==0.0.8->luxai_s3) (0.1.65)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium->gymnax==0.0.8->luxai_s3) (0.0.4)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (1.3.0)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (4.53.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (24.1)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (10.4.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gymnax==0.0.8->luxai_s3) (2.8.2)\nRequirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn->gymnax==0.0.8->luxai_s3) (2.1.4)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->luxai_s3) (0.1.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn->gymnax==0.0.8->luxai_s3) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn->gymnax==0.0.8->luxai_s3) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->gymnax==0.0.8->luxai_s3) (1.16.0)\nRequirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (1.9.4)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (1.6.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (3.20.3)\nRequirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (4.10.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (2024.6.1)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (6.4.5)\nRequirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax->gymnax==0.0.8->luxai_s3) (3.20.2)\nDownloading luxai_s3-0.2.0-py3-none-any.whl (35 kB)\nDownloading gymnax-0.0.8-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.3/96.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tyro-0.9.11-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827696 sha256=774af28a60fe38f63cb0299244da0c608e20b828b4e0e0dc5b8d91e11679399c\n  Stored in directory: /root/.cache/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\nSuccessfully built gym\nInstalling collected packages: shtab, gym, tyro, gymnax, luxai_s3\n  Attempting uninstall: gym\n    Found existing installation: gym 0.25.2\n    Uninstalling gym-0.25.2:\n      Successfully uninstalled gym-0.25.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.0.9 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gym-0.26.2 gymnax-0.0.8 luxai_s3-0.2.0 shtab-1.7.1 tyro-0.9.11\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/lux-s3/lux-ai-season-3/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T18:47:16.238051Z","iopub.execute_input":"2025-01-21T18:47:16.238414Z","iopub.status.idle":"2025-01-21T18:47:16.243173Z","shell.execute_reply.started":"2025-01-21T18:47:16.238384Z","shell.execute_reply":"2025-01-21T18:47:16.242127Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class Actor(nn.Module):\n    def __init__(self,inp,out):\n        super(Actor,self).__init__()\n        self.fc_layer = nn.Sequential(\n            nn.Linear(inp, 256),\n            nn.LayerNorm(256),  \n            nn.ReLU(),\n            nn.Dropout(0.1), \n            \n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(128, 64),\n            nn.LayerNorm(64),\n            nn.ReLU(),\n            \n            nn.Linear(64, out),\n        )\n        for layer in self.fc_layer:\n            if isinstance(layer, nn.Linear):\n                nn.init.orthogonal_(layer.weight, gain=0.1)\n                nn.init.zeros_(layer.bias)\n        self.fc_layer[-1].bias.data[0] = -0.7 \n        self.fc_layer[-1].bias.data[1:] = 0.1\n    def forward(self,x):\n        x = x.unsqueeze(0)\n        out=self.fc_layer(x).squeeze()\n        temperature = 1.5  \n        return F.softmax(out / temperature, dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T18:47:16.272964Z","iopub.execute_input":"2025-01-21T18:47:16.273340Z","iopub.status.idle":"2025-01-21T18:47:16.293867Z","shell.execute_reply.started":"2025-01-21T18:47:16.273312Z","shell.execute_reply":"2025-01-21T18:47:16.292740Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Value(nn.Module):\n    def __init__(self,inp,out,hid,num_layer=3):\n        super(Value,self).__init__()\n        self.fc_layer=nn.Sequential(\n            nn.Linear(inp, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),   \n            \n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(128, 64),\n            nn.LayerNorm(64),\n            nn.ReLU(),\n            \n            nn.Linear(64, out),)\n                        \n    def forward(self,x):\n        x = x.unsqueeze(0)\n        out=self.fc_layer(x)\n        return out\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T18:47:16.294955Z","iopub.execute_input":"2025-01-21T18:47:16.295268Z","iopub.status.idle":"2025-01-21T18:47:16.321676Z","shell.execute_reply.started":"2025-01-21T18:47:16.295239Z","shell.execute_reply":"2025-01-21T18:47:16.320615Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from luxai_s3.wrappers import LuxAIS3GymEnv\ndef train(epoch=10,batch=50,seed=np.random.seed()):\n    replay_save_dir=\"replays\"\n    env = RecordEpisode(LuxAIS3GymEnv(numpy_output=True), save_on_reset=True, save_dir=replay_save_dir)\n    obs, info = env.reset(seed=seed)\n    env_cfg = info[\"params\"]\n    agent0=Agent(\"player_0\",env_cfg)\n    agent1=Agent(\"player_1\",env_cfg) \n    action={}\n    training=True\n    for i in range(epoch):\n        print('epoch :',i)\n        obs, info = env.reset()\n        done=False\n        step=0\n        game_done=False\n        last_action={}\n        last_last_action={}\n        for agent in [agent0,agent1]:\n            agent.reset_episode()\n            last_action[agent.player]=agent.last_action\n            last_last_action[agent.player]=last_action[agent.player]\n        k=0\n        last_obs=None\n        last_rew = {\n                \"player_0\": 0,\n                \"player_1\": 0\n            } \n        while (not game_done) and k<batch:\n            # print(f\"k : {k}\")\n            last_obs = {\n                        \"player_0\": obs[\"player_0\"].copy(),\n                        \"player_1\": obs[\"player_1\"].copy()\n            }\n            prob={}\n            val={}\n            for agent in [agent0,agent1]:\n                action[agent.player], prob[agent.player],val[agent.player]=agent.t_act(step,obs[agent.player],epoch,i)\n                \n            obs, reward, terminated, truncated, info=env.step(action)\n            \n            dones = {k: terminated[k] | truncated[k] for k in terminated}\n            rew = {\n                \"player_0\": obs[\"player_0\"][\"team_points\"][agent0.team_id],\n                \"player_1\": obs[\"player_1\"][\"team_points\"][agent1.team_id]\n            } \n            pen={}\n            pen[\"player_0\"]=4*(rew[\"player_0\"]- last_rew[\"player_0\"]) if rew[\"player_0\"]- last_rew[\"player_0\"]!=0 else -0.05\n            pen[\"player_1\"]=4*(rew[\"player_1\"]- last_rew[\"player_1\"]) if rew[\"player_1\"]- last_rew[\"player_1\"]!=0 else -0.01\n            rewards={\n                \"player_0\": pen[\"player_0\"],\n                \"player_1\": pen[\"player_1\"]\n            }\n            last_rew={\n                \"player_0\": rew[\"player_0\"],\n                \"player_1\": rew[\"player_1\"]\n            } \n            if training:\n                for agent in [agent0,agent1]:\n                  \n                    for unit_id in range(env_cfg['max_units']):\n                        if obs[agent.player]['units_mask'][agent.team_id][unit_id]:\n                            if np.array_equal(obs[agent.player][\"units\"][\"position\"][agent.team_id][unit_id], [-1, -1]):\n                                print(obs[agent.player][\"units\"][\"position\"][agent.team_id][unit_id])\n                            act=action[agent.player][unit_id][0]\n                            last_state=agent.rep(last_obs[agent.player][\"units\"][\"position\"][agent.team_id][unit_id],last_obs[agent.player][\"relic_nodes\"],last_obs[agent.player][\"units\"][\"energy\"][agent.team_id][unit_id],step,last_obs[agent.player][\"relic_nodes_mask\"],last_last_action[agent.player][unit_id])\n                            state=agent.rep(obs[agent.player][\"units\"][\"position\"][agent.team_id][unit_id],obs[agent.player][\"relic_nodes\"],obs[agent.player][\"units\"][\"energy\"][agent.team_id][unit_id],step,obs[agent.player][\"relic_nodes_mask\"],last_action[agent.player][unit_id])\n                            rewards[agent.player] += 3 * (1.0 / ((state[6]) + 1.0))                            \n                            pos_tuple = tuple(obs[agent.player][\"units\"][\"position\"][agent.team_id][unit_id])\n                            if pos_tuple not in agent.visited_positions:\n                                rewards[agent.player] += 0.5\n                                agent.visited_positions.add(pos_tuple)\n                            if last_state[0]==state[0] and last_state[1]==state[1] and pen[agent.player]<=0:\n                                rewards[agent.player]-=0.07\n                            last_last_action[agent.player]=last_action[agent.player]\n                            last_action[agent.player]=agent.last_action\n                            agent.traj.append((last_state,act,rewards[agent.player],state,prob[agent.player][unit_id],done,val[agent.player][unit_id]))\n                            \n            step+=1\n            k+=1              \n            if dones[\"player_0\"] or dones[\"player_1\"]:\n                    game_done = True\n            if training:\n                agent0.save_model()\n                agent1.save_model()\n        agent0.teach()\n        agent1.teach()\n        if((i+1)%10==0):\n            render_episode(env)\n    agent0.plot_entropy()\n    env.close()\n    return agent0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T19:17:19.979011Z","iopub.execute_input":"2025-01-21T19:17:19.979408Z","iopub.status.idle":"2025-01-21T19:17:19.998403Z","shell.execute_reply.started":"2025-01-21T19:17:19.979377Z","shell.execute_reply":"2025-01-21T19:17:19.997125Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"from lux.utils import direction_to\nimport sys\nimport numpy as np\nclass Agent():\n    def __init__(self, player: str, env_cfg) -> None:\n        self.player = player\n        self.opp_player = \"player_1\" if self.player == \"player_0\" else \"player_0\"\n        self.team_id = 0 if self.player == \"player_0\" else 1\n        self.opp_team_id = 1 if self.team_id == 0 else 0\n        self.seed=np.random.seed(0)\n        self.env_cfg = env_cfg\n        self.batch_size=10\n        self.gamma=0.99\n        self.clip_param = 0.2\n        self.relic_node_positions = []\n        self.discovered_relic_nodes_ids = set()\n        self.entropy_history=[]\n        self.visited_positions = set()\n        self.unit_explore_locations = dict()\n        self.explore_loc_update=np.zeros(self.env_cfg[\"max_units\"], dtype=float)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.policy = Actor(9, 5).to(self.device)\n        self.value = Value(9, 1, 5, 5).to(self.device)\n        self.policy_opti = torch.optim.Adam(self.policy.parameters(), lr=1e-4)\n        self.value_opti = torch.optim.Adam(self.value.parameters(), lr=1e-4)\n        self.traj=[]\n        self.last_action=np.zeros(self.env_cfg[\"max_units\"], dtype=int)\n\n    def reset_episode(self):\n        self.relic_node_positions = []\n        self.last_action=np.zeros(self.env_cfg[\"max_units\"], dtype=int)\n        self.discovered_relic_nodes_ids = set()\n        self.visited_positions = set()\n        self.unit_explore_locations = dict()\n        self.explore_loc_update=np.zeros(self.env_cfg[\"max_units\"], dtype=float)\n    def save_model(self):\n        torch.save({\n            'policy_net': self.policy.state_dict(),\n            'policy_opti': self.policy_opti.state_dict()\n        }, f'dqn_model_{self.player}.pth')\n        \n    def rep(self,unit_pos,relic_nodes,unit_energy,step,relic_mask,last_act):\n        visible=relic_nodes[relic_mask]\n        direc=-1\n        dist=1000\n        if relic_mask.any():\n            dist=np.linalg.norm(visible-unit_pos,axis=1)\n            closest=visible[np.argmin(dist)]\n\n            dist=dist.min()\n            direc=direction_to(unit_pos, closest)\n        else:\n            closest=np.array([-1,-1])\n\n        return torch.FloatTensor(np.concatenate([unit_pos,closest,[direc],[unit_energy/100],[dist],[last_act],[step/505]])).to(self.device)\n        \n    def act(self, step: int, obs, remainingOverageTime: int = 60):\n   \n        unit_mask = np.array(obs[\"units_mask\"][self.team_id])  # shape (max_units,)\n        unit_positions = np.array(obs[\"units\"][\"position\"][self.team_id])  # shape (max_units, 2)\n        unit_energys = np.array(obs[\"units\"][\"energy\"][self.team_id])  # shape (max_units, 1)\n        observed_relic_node_positions = np.array(obs[\"relic_nodes\"])  # shape (max_relic_nodes, 2)\n        observed_relic_nodes_mask = np.array(obs[\"relic_nodes_mask\"])  # shape (max_relic_nodes,)\n        team_points = np.array(obs[\"team_points\"])  # Points scored by each team\n    \n        # Ids of controllable units at this timestep\n        available_unit_ids = np.where(unit_mask)[0]\n    \n        # Visible relic nodes\n        visible_relic_node_ids = set(np.where(observed_relic_nodes_mask)[0])\n    \n        actions = np.zeros((self.env_cfg[\"max_units\"], 3), dtype=int)\n    \n        # Save new relic nodes discovered\n        for id in visible_relic_node_ids:\n            if id not in self.discovered_relic_nodes_ids:\n                self.discovered_relic_nodes_ids.add(id)\n                self.relic_node_positions.append(observed_relic_node_positions[id])\n                \n        with torch.no_grad():\n            for units_id in available_unit_ids:\n                unit_pos=unit_positions[units_id]\n                state_rep= self.rep(unit_pos,observed_relic_node_positions,unit_energys[units_id],step,observed_relic_nodes_mask)\n                log_prob=self.policy.forward(state_rep)\n                act=torch.argmax((log_prob))\n                actions[units_id]=[act,0,0]\n        return actions\n        \n    def compute_intrinsic_reward(self, state):\n        state_tensor = torch.FloatTensor(state).to(self.device)\n        with torch.no_grad():\n            value_pred = self.value(state_tensor)\n        return 0.01 * torch.abs(value_pred).item() \n\n    def plot_entropy(self):\n        import matplotlib.pyplot as plt\n        \n        plt.figure(figsize=(10, 6))\n        plt.plot(self.entropy_history)\n        plt.title('Policy Entropy over Training')\n        plt.xlabel('Training Steps')\n        plt.ylabel('Entropy')\n        plt.grid(True)\n        \n        # Add horizontal line at maximum possible entropy for 5 actions\n        max_entropy = -np.log(1/5)  # ≈ 1.61\n        plt.axhline(y=max_entropy, color='r', linestyle='--', \n                    label=f'Max Entropy ({max_entropy:.2f})')\n        \n        plt.legend()\n        plt.show()\n        \n    def get_valid_actions(self, unit_pos, unit_energy, state_rep):\n        valid = np.ones(5)  \n\n        if state_rep[2]==-1 and state_rep[3]==-1:\n            valid[0] = 0.7 \n        if unit_pos[1] <= 0: \n            valid[1] = 0\n        if unit_pos[0] >= self.env_cfg[\"map_width\"]-1: \n            valid[2] = 0\n        if unit_pos[1] >= self.env_cfg[\"map_height\"]-1:  \n            valid[3] = 0\n        if unit_pos[0] <= 0: \n            valid[4] = 0\n        \n        return torch.FloatTensor(valid).to(self.device)\n    \n    def t_act(self, step: int, obs,ep, curr, remainingOverageTime: int = 60,):\n   \n        unit_mask = np.array(obs[\"units_mask\"][self.team_id])  # shape (max_units,)\n        unit_positions = np.array(obs[\"units\"][\"position\"][self.team_id])  # shape (max_units, 2)\n        unit_energys = np.array(obs[\"units\"][\"energy\"][self.team_id])  # shape (max_units, 1)\n        observed_relic_node_positions = np.array(obs[\"relic_nodes\"])  # shape (max_relic_nodes, 2)\n        observed_relic_nodes_mask = np.array(obs[\"relic_nodes_mask\"])  # shape (max_relic_nodes,)\n        team_points = np.array(obs[\"team_points\"])  # Points scored by each team\n    \n        # Ids of controllable units at this timestep\n        available_unit_ids = np.where(unit_mask)[0]\n    \n        # Visible relic nodes\n        visible_relic_node_ids = set(np.where(observed_relic_nodes_mask)[0])\n    \n        actions = np.zeros((self.env_cfg[\"max_units\"], 3), dtype=int)\n        prob = np.zeros(self.env_cfg[\"max_units\"], dtype=float)\n        val_col = np.zeros(self.env_cfg[\"max_units\"], dtype=float)\n    \n        # Save new relic nodes discovered\n        for id in visible_relic_node_ids:\n            if id not in self.discovered_relic_nodes_ids:\n                self.discovered_relic_nodes_ids.add(id)\n                self.relic_node_positions.append(observed_relic_node_positions[id])\n           \n        with torch.no_grad():\n\n            for units_id in available_unit_ids:\n                unit_pos=unit_positions[units_id]\n                state_rep= self.rep(unit_pos,observed_relic_node_positions,unit_energys[units_id],step/505.0,observed_relic_nodes_mask,self.last_action[units_id])\n                valid_actions=self.get_valid_actions(unit_pos,unit_energys[units_id],state_rep)\n                log_prob=self.policy.forward(state_rep).squeeze()\n                val=self.value.forward(state_rep)\n                if (np.random.random() < max(0,0)) and curr!=49:\n                    if len(self.relic_node_positions) > 0:\n                        nearest_relic_node = self.relic_node_positions[0]\n                        distance_to_relic = abs(unit_pos[0] - nearest_relic_node[0]) + abs(unit_pos[1] - nearest_relic_node[1])\n                        if distance_to_relic <= 4:\n                            random_direction = np.random.randint(0, 5)\n                            act = torch.tensor(random_direction)\n                        else:\n                            act= torch.tensor( direction_to(unit_pos, nearest_relic_node))\n                    else:\n                        if self.explore_loc_update[units_id]==0 or step-self.explore_loc_update[units_id] >=20:\n                            random_loc=(np.random.randint(0, self.env_cfg[\"map_width\"]), np.random.randint(0, self.env_cfg[\"map_height\"]))\n                            self.unit_explore_locations[units_id]=random_loc\n                        act=torch.tensor(direction_to(unit_pos, self.unit_explore_locations[units_id]))\n                else:\n                    act = torch.argmax(log_prob)\n                self.last_action[units_id]=act.cpu().item()\n                actions[units_id]=[act.cpu().item(),0,0]\n                prob[units_id]=(log_prob[act])\n                val_col[units_id]=val.item()\n        return actions, prob , val_col \n\n    def teach(self):\n        T=len(self.traj)\n        value_col=np.zeros(T)\n        return_col=np.zeros(T)\n        prob_col=np.zeros(T)\n        reward_col=np.zeros(T)\n        last_state=0\n        states=[]\n        actions=[]\n        i=0\n        for e in self.traj:\n            state,act,reward,next_state,log_prob,done,val=e\n            value_col[i]=(val)\n            states.append(state)\n            actions.append(act)\n            entropy_reward=self.compute_intrinsic_reward(state)\n            prob_col[i]=(log_prob)\n            reward_col[i]=(reward)+(0.01*entropy_reward)\n            last_state=next_state\n            i+=1\n        states = torch.stack(states).to(self.device)\n        actions = torch.tensor(actions).to(self.device)\n        return_col[-1]=self.value.forward(self.traj[-1][3]).item()\n        print(reward_col)\n        for t in reversed(range(T-1)):\n            return_col[t]= reward_col[t]+ self.gamma*return_col[t+1]\n        return_col = (return_col - return_col.mean()) / (return_col.std() + 1e-8)\n        adv_col=return_col-value_col\n        adv_col = (adv_col - adv_col.mean()) / (adv_col.std() + 1e-8)\n        adv_col = torch.FloatTensor(adv_col).to(self.device)\n        return_col = torch.FloatTensor(return_col).to(self.device)\n        for _ in range(30):\n            new_prob=(torch.clamp(self.policy.forward(states), min=1e-8)).to(self.device)\n            prob_col=torch.clamp(torch.FloatTensor(prob_col).to(self.device),min=1e-8)\n            entropy = (new_prob * torch.log(new_prob + 1e-8)).sum(dim=-1)\n            mean_entropy = entropy.mean().item()\n            entropy_bonus = -0.08*mean_entropy\n            \n            if not hasattr(self, 'entropy_history'):\n                self.entropy_history = []\n            self.entropy_history.append(mean_entropy)\n            \n            new_prob= new_prob.gather(1,actions.unsqueeze(1)).squeeze()\n            ratio = (new_prob/prob_col)\n            clipped_ratio = torch.clamp(ratio, 1-self.clip_param, 1+self.clip_param)\n            policy_loss=-torch.min((ratio * adv_col),clipped_ratio* adv_col).mean() +entropy_bonus\n            value_loss= nn.MSELoss()(self.value(states).squeeze().to(self.device),return_col)\n            self.policy_opti.zero_grad()\n            policy_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n            self.policy_opti.step()\n            self.value_opti.zero_grad()\n            value_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.value.parameters(), 0.5)\n            self.value_opti.step()\n        print(f\"policy {self.player}: {policy_loss}  value {self.player}: {value_loss}\")\n        self.traj.clear()\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T20:51:50.951043Z","iopub.execute_input":"2025-01-21T20:51:50.951634Z","iopub.status.idle":"2025-01-21T20:51:50.996256Z","shell.execute_reply.started":"2025-01-21T20:51:50.951596Z","shell.execute_reply":"2025-01-21T20:51:50.995020Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import json\nfrom IPython.display import display, Javascript\nfrom luxai_s3.wrappers import LuxAIS3GymEnv, RecordEpisode\ndef render_episode(episode: RecordEpisode) -> None:\n    data = json.dumps(episode.serialize_episode_data(), separators=(\",\", \":\"))\n    display(Javascript(f\"\"\"\nvar iframe = document.createElement('iframe');\niframe.src = 'https://s3vis.lux-ai.org/#/kaggle';\niframe.width = '100%';\niframe.scrolling = 'no';\n\niframe.addEventListener('load', event => {{\n    event.target.contentWindow.postMessage({data}, 'https://s3vis.lux-ai.org');\n}});\n\nnew ResizeObserver(entries => {{\n    for (const entry of entries) {{\n        entry.target.height = `${{Math.round(320 + 0.3 * entry.contentRect.width)}}px`;\n    }}\n}}).observe(iframe);\n\nelement.append(iframe);\n    \"\"\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T20:51:53.729508Z","iopub.execute_input":"2025-01-21T20:51:53.729897Z","iopub.status.idle":"2025-01-21T20:51:53.735487Z","shell.execute_reply.started":"2025-01-21T20:51:53.729866Z","shell.execute_reply":"2025-01-21T20:51:53.734368Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"ag=train(500,505)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ag.plot_entropy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T19:16:31.238530Z","iopub.status.idle":"2025-01-21T19:16:31.239027Z","shell.execute_reply":"2025-01-21T19:16:31.238830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/replays\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-21T19:16:31.240073Z","iopub.status.idle":"2025-01-21T19:16:31.240565Z","shell.execute_reply":"2025-01-21T19:16:31.240338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}