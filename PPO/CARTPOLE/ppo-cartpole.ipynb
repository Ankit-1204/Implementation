{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:47:15.203617Z","iopub.execute_input":"2025-01-23T14:47:15.203979Z","iopub.status.idle":"2025-01-23T14:47:15.211717Z","shell.execute_reply.started":"2025-01-23T14:47:15.203950Z","shell.execute_reply":"2025-01-23T14:47:15.210620Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"import gymnasium as gym\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nfrom gymnasium.wrappers import RecordEpisodeStatistics, RecordVideo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:47:17.163040Z","iopub.execute_input":"2025-01-23T14:47:17.163386Z","iopub.status.idle":"2025-01-23T14:47:17.169644Z","shell.execute_reply.started":"2025-01-23T14:47:17.163358Z","shell.execute_reply":"2025-01-23T14:47:17.168268Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"class Actor(nn.Module):\n    def __init__(self,inp,out):\n        super(Actor,self).__init__()\n        self.fc_layer = nn.Sequential(\n            nn.Linear(inp, 256),\n            nn.LayerNorm(256),  \n            \n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.Dropout(0.1),\n            \n            nn.Linear(128, 64),\n            nn.LayerNorm(64),\n            \n            nn.Linear(64, out),\n            nn.Softmax()\n        )\n    def forward(self,x):\n        out=self.fc_layer(torch.tensor(x))\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:47:52.786047Z","iopub.execute_input":"2025-01-23T14:47:52.786386Z","iopub.status.idle":"2025-01-23T14:47:52.793252Z","shell.execute_reply.started":"2025-01-23T14:47:52.786359Z","shell.execute_reply":"2025-01-23T14:47:52.791571Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"class Value(nn.Module):\n    def __init__(self,inp,out):\n        super(Value,self).__init__()\n        self.fc_layer=nn.Sequential(\n            nn.Linear(inp, 256),\n            nn.LayerNorm(256),  \n            \n            nn.Linear(256, 128),\n            nn.LayerNorm(128),\n            nn.Dropout(0.1),\n            \n            nn.Linear(128, 64),\n            nn.LayerNorm(64),\n            \n            nn.Linear(64, out),\n            nn.Softmax())\n                        \n    def forward(self,x):\n        x=torch.tensor(x)\n        out=self.fc_layer(x)\n        return out\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T14:47:56.007219Z","iopub.execute_input":"2025-01-23T14:47:56.007697Z","iopub.status.idle":"2025-01-23T14:47:56.014295Z","shell.execute_reply.started":"2025-01-23T14:47:56.007660Z","shell.execute_reply":"2025-01-23T14:47:56.013026Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"import sys\nclass Agent():\n    def __init__(self) -> None:\n\n        self.seed=np.random.seed(0)\n        self.batch_size=10\n        self.gamma=0.99\n        self.lamda=0.95\n        self.clip_param = 0.2\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.policy = Actor(4, 2).to(self.device)\n        self.value = Value(4, 1).to(self.device)\n        self.policy_opti = torch.optim.Adam(self.policy.parameters(), lr=1e-3)\n        self.value_opti = torch.optim.Adam(self.value.parameters(), lr=1e-4)\n        self.traj=[]\n\n    def save_model(self,epoch):\n        torch.save({\n            'policy_net': self.policy.state_dict(),\n            'policy_opti': self.policy_opti.state_dict()\n        }, f'dqn_model_epoch {epoch}.pth')\n        \n    def act(self, state):\n        with torch.no_grad():\n            out=(self.policy.forward(state))\n            val=self.value.forward(state)\n            return val.detach(), out.detach()\n   \n    \n    def teach(self):\n        T=len(self.traj)\n        value_col=np.zeros(T+1)\n        return_col=np.zeros(T)\n        advantage=np.zeros(T)\n        prob_col=np.zeros(T)\n        reward_col=np.zeros(T)\n        dones=np.zeros(T)\n        last_state=0\n        states=[]\n        actions=[]\n        i=0\n        for e in self.traj:\n            state,act,reward,next_state,log_prob,done,val=e\n            value_col[i]=(val)\n            dones[i]=done\n            states.append(torch.tensor(state))\n            actions.append(torch.tensor(act))\n            prob_col[i]=(log_prob)\n            reward_col[i]=(reward)\n            last_state=next_state\n            i+=1\n        \n        states = torch.stack(states).to(self.device)\n        actions = torch.tensor(actions).to(self.device)\n        value_col[-1]=self.value.forward(self.traj[-1][3]).item()\n        gae=0\n        for t in reversed(range(T)):\n            delta = reward_col[t] + self.gamma * value_col[t + 1] * (1 - dones[t]) - value_col[t]\n            gae = delta + self.gamma * self.lamda * gae * (1 - dones[t])\n            advantage[t] = gae\n            return_col[t] = gae + value_col[t]\n        advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n        reward_col = (reward_col - reward_col.mean()) / (reward_col.std() + 1e-8)\n        return_col = (return_col - return_col.mean()) / (return_col.std() + 1e-8)\n        adv_col=torch.FloatTensor(advantage).to(self.device)\n        return_col = torch.FloatTensor(return_col).to(self.device)\n        \n        for _ in range(10):\n            new_prob=(torch.clamp(self.policy.forward(states), min=1e-8)).to(self.device)\n            prob_col=torch.clamp(torch.tensor(prob_col).to(self.device),min=1e-8)\n    \n            dist=Categorical(new_prob)\n            new_log_prob=dist.log_prob(actions)\n            ratio = torch.exp(new_log_prob-prob_col)\n            clipped_ratio = torch.clamp(ratio, 1-self.clip_param, 1+self.clip_param)\n            policy_loss=-torch.min((ratio * adv_col),clipped_ratio* adv_col).mean() \n            value_loss= nn.MSELoss()(self.value(states).squeeze().to(self.device),return_col)\n            if torch.isnan(policy_loss).any() or torch.isinf(policy_loss).any() or torch.isnan(value_loss).any() or torch.isinf(value_loss).any():\n                print(\"NaN or Inf in policy_loss\")\n                break\n            self.policy_opti.zero_grad()\n            policy_loss.backward()\n            self.policy_opti.step()\n            self.value_opti.zero_grad()\n            value_loss.backward()\n            self.value_opti.step()\n        self.traj.clear()\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T15:06:00.332050Z","iopub.execute_input":"2025-01-23T15:06:00.332396Z","iopub.status.idle":"2025-01-23T15:06:00.349140Z","shell.execute_reply.started":"2025-01-23T15:06:00.332370Z","shell.execute_reply":"2025-01-23T15:06:00.348006Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"def train(epoch, batch=20):\n    agent=Agent()\n    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n    # env = RecordVideo(env, video_folder=\"cartpole-agent\", name_prefix=\"eval\",\n    #               episode_trigger=lambda x: True)\n    k=0\n    for i in tqdm(range(epoch), desc=\"Training Epochs\"):\n        state,info=env.reset()\n        episode_done=False\n        step=0\n        true_rew=0\n        while not episode_done and step<500 :\n            val, prob=agent.act(state)\n            dist = torch.distributions.Categorical(probs=prob)\n            action=dist.sample()\n            log_prob=dist.log_prob(action)\n            last_state=state\n            state, reward, terminated, truncated, info = env.step(action.item() )\n            if terminated or truncated:\n                episode_done=True;\n            agent.traj.append((last_state,action,reward,state,log_prob,episode_done,val))\n            k+=1\n            true_rew+=reward\n            step+=1\n            if(k>=batch):\n                k=0\n                agent.teach()\n        if(i%100==0):\n            print(true_rew)\n    env.close()\n    return agent\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T15:06:02.326056Z","iopub.execute_input":"2025-01-23T15:06:02.326390Z","iopub.status.idle":"2025-01-23T15:06:02.334092Z","shell.execute_reply.started":"2025-01-23T15:06:02.326364Z","shell.execute_reply":"2025-01-23T15:06:02.332786Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"agent=train(3000,512)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T15:10:42.128891Z","iopub.execute_input":"2025-01-23T15:10:42.129266Z","iopub.status.idle":"2025-01-23T15:15:20.024071Z","shell.execute_reply.started":"2025-01-23T15:10:42.129233Z","shell.execute_reply":"2025-01-23T15:15:20.022974Z"}},"outputs":[{"name":"stderr","text":"Training Epochs:   0%|          | 14/3000 [00:00<00:44, 67.11it/s]","output_type":"stream"},{"name":"stdout","text":"20.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:   1%|          | 28/3000 [00:00<00:51, 57.21it/s]<ipython-input-103-e4e3664ffaed>:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  actions.append(torch.tensor(act))\n<ipython-input-84-ba2cd78712bc>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  out=self.fc_layer(torch.tensor(x))\n<ipython-input-103-e4e3664ffaed>:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  prob_col=torch.clamp(torch.tensor(prob_col).to(self.device),min=1e-8)\nTraining Epochs:   4%|▎         | 107/3000 [00:02<01:18, 36.93it/s]","output_type":"stream"},{"name":"stdout","text":"16.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:   7%|▋         | 207/3000 [00:05<01:19, 35.31it/s]","output_type":"stream"},{"name":"stdout","text":"24.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  10%|█         | 308/3000 [00:08<01:12, 36.90it/s]","output_type":"stream"},{"name":"stdout","text":"19.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  14%|█▎        | 408/3000 [00:11<01:06, 39.09it/s]","output_type":"stream"},{"name":"stdout","text":"23.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  17%|█▋        | 503/3000 [00:14<01:06, 37.80it/s]","output_type":"stream"},{"name":"stdout","text":"15.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  20%|██        | 606/3000 [00:17<01:27, 27.31it/s]","output_type":"stream"},{"name":"stdout","text":"25.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  23%|██▎       | 701/3000 [00:23<04:45,  8.05it/s]","output_type":"stream"},{"name":"stdout","text":"104.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  27%|██▋       | 802/3000 [00:43<08:15,  4.43it/s]","output_type":"stream"},{"name":"stdout","text":"170.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  30%|███       | 912/3000 [00:54<00:31, 66.58it/s]","output_type":"stream"},{"name":"stdout","text":"13.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  34%|███▎      | 1012/3000 [00:55<00:24, 81.01it/s]","output_type":"stream"},{"name":"stdout","text":"11.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  37%|███▋      | 1115/3000 [00:57<00:25, 72.59it/s]","output_type":"stream"},{"name":"stdout","text":"11.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  40%|████      | 1214/3000 [00:58<00:26, 67.42it/s]","output_type":"stream"},{"name":"stdout","text":"12.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  44%|████▎     | 1312/3000 [01:00<00:20, 81.80it/s]","output_type":"stream"},{"name":"stdout","text":"11.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  47%|████▋     | 1414/3000 [01:01<00:19, 79.77it/s]","output_type":"stream"},{"name":"stdout","text":"11.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  50%|█████     | 1515/3000 [01:03<00:19, 77.29it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  54%|█████▍    | 1618/3000 [01:04<00:17, 80.27it/s]","output_type":"stream"},{"name":"stdout","text":"11.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  57%|█████▋    | 1717/3000 [01:05<00:16, 79.47it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  60%|██████    | 1813/3000 [01:07<00:16, 74.01it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  64%|██████▍   | 1913/3000 [01:08<00:14, 77.21it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  67%|██████▋   | 2017/3000 [01:10<00:12, 75.78it/s]","output_type":"stream"},{"name":"stdout","text":"12.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  71%|███████   | 2116/3000 [01:11<00:11, 77.52it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  74%|███████▎  | 2212/3000 [01:13<00:10, 74.47it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  77%|███████▋  | 2317/3000 [01:14<00:08, 79.24it/s]","output_type":"stream"},{"name":"stdout","text":"9.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  80%|████████  | 2408/3000 [01:16<00:07, 76.59it/s]","output_type":"stream"},{"name":"stdout","text":"10.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  83%|████████▎ | 2498/3000 [01:17<00:07, 66.38it/s]","output_type":"stream"},{"name":"stdout","text":"12.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  87%|████████▋ | 2602/3000 [01:24<01:13,  5.38it/s]","output_type":"stream"},{"name":"stdout","text":"140.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  90%|█████████ | 2701/3000 [01:44<00:58,  5.08it/s]","output_type":"stream"},{"name":"stdout","text":"147.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  93%|█████████▎| 2801/3000 [02:24<01:55,  1.72it/s]","output_type":"stream"},{"name":"stdout","text":"342.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs:  97%|█████████▋| 2901/3000 [03:32<01:05,  1.51it/s]","output_type":"stream"},{"name":"stdout","text":"500.0\n","output_type":"stream"},{"name":"stderr","text":"Training Epochs: 100%|██████████| 3000/3000 [04:37<00:00, 10.80it/s]\n","output_type":"stream"}],"execution_count":108},{"cell_type":"code","source":"agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T15:51:44.678330Z","iopub.execute_input":"2025-01-23T15:51:44.681008Z","iopub.status.idle":"2025-01-23T15:51:44.698343Z","shell.execute_reply.started":"2025-01-23T15:51:44.680858Z","shell.execute_reply":"2025-01-23T15:51:44.696917Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"<__main__.Agent at 0x7f7408d02170>"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\nenv = RecordVideo(env, video_folder=\"cartpole-agent\", name_prefix=\"eval\",\n                  episode_trigger=lambda x: True)\nstate,_=env.reset()\nepisode_done=False\ntrue_rew=0\nwhile not episode_done:\n    val, prob=agent.act(state)\n    print(prob)\n    dist = torch.distributions.Categorical(probs=prob)\n    action=dist.sample()\n    log_prob=dist.log_prob(action)\n    last_state=state\n    state, reward, terminated, truncated, info = env.step(action.item() )\n    true_rew+=reward\n    if terminated :\n        episode_done=True;\nprint(true_rew)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}